{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークと実装プログラム解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの初歩的解説です。ディープラーニングの理論に興味がある方は学習に挑戦してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配降下法\n",
    "\n",
    "説明変数$x$から、目的変数$y$を予測したいとします。\n",
    "\n",
    "これを非常にシンプルな線形な関数$f(x) = ax+b$でモデル化するとします。\n",
    "\n",
    "仮に、$x$と$y$が$y = 2x+3$の関係であったとしましょう。\n",
    "\n",
    "我々は本当の値$a=2, b=3$を知りません。しかし、観測された一部のサンプルが存在するとします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random():\n",
    "    x = np.random.rand(50)\n",
    "    noise = np.random.rand(50) / 10\n",
    "    y = 2 * x + 3 + noise\n",
    "    return zip(x,y)\n",
    "\n",
    "# ランダムにノイズの乗った50個のサンプルを作成します\n",
    "sample = random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sample_xs, sample_ys = np.asarray(sample).transpose()\n",
    "plt.scatter(sample_xs, sample_ys, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の上記のランダムに生成したデータは、人の目で見れば直線が簡単に予測できますね。これを機械的に学習するのが機械学習です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しに、$a=0, b=0$で初期化して、一致するか試してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b, x):\n",
    "    return a * x + b\n",
    "\n",
    "for x, y in sample[:10]:\n",
    "    print('{} <==> {}'.format(f(0,0,x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然、一致しません。しかし、サンプルデータを基に、なんとか本当の$a,b$を学習してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、勾配降下法を用いてみましょう。\n",
    "\n",
    "まず、サンプルデータに対する予測結果と、サンプルの答えがどの程度違ったかを定量的に表します。これを誤差と呼びます。\n",
    "\n",
    "そして、各パラメータを少しずらした時に誤差が小さくなるように、各パラメータを更新します。\n",
    "\n",
    "具体的には、各パラメータ$w_i$を、誤差$E$に対する偏微分値を引いた値で更新します。\n",
    "\n",
    "$$ w_i \\leftarrow w_i - \\lambda{\\partial E \\over \\partial w_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/b.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差を2乗誤差${1 \\over 2}(y - f(x))^2$とし、パラメータ$a,b$それぞれに対する偏微分を計算して値を更新します。\n",
    "\n",
    "$a,b$の誤差に対する偏微分は以下のようになります。\n",
    "\n",
    "$${\\partial E \\over \\partial a}={\\partial E \\over \\partial f}{\\partial f \\over \\partial a}=(f(x)-y)x$$\n",
    "$${\\partial E \\over \\partial b}={\\partial E \\over \\partial f}{\\partial f \\over \\partial b}=f(x)-y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b, x):\n",
    "    return a * x + b\n",
    "\n",
    "def loss(y, pred):\n",
    "    return ((y - pred) * (y - pred))/2\n",
    "\n",
    "a = 0\n",
    "b = 0\n",
    "lr = 0.1 # 学習率\n",
    "for i in range(10):\n",
    "    print('a={} b={}'.format(a,b))\n",
    "    for x, y in sample:\n",
    "        pred = f(a,b,x)\n",
    "        E = loss(y, pred) # 二乗誤差\n",
    "        \n",
    "        # 誤差が小さくなるよう、パラメータに対する偏微分値でパラメータを更新\n",
    "        a = a - lr * ((pred - y)*x)\n",
    "        b = b - lr * (pred - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sample_xs, sample_ys, s=2)\n",
    "_xs = np.linspace(0,1,10)\n",
    "_ys = [f(a,b,x) for x in _xs]\n",
    "plt.plot(_xs, _ys, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おそらくほぼ$a=2,b=3$になったのではないかと思います。これが勾配降下法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlowで例を実装する\n",
    "\n",
    "一次関数の学習をTensorFlowで実装してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    zero = tf.constant(0., dtype=tf.float64)\n",
    "    \n",
    "    # 引数\n",
    "    x = tf.placeholder(tf.float64, name='x') # 入力\n",
    "    y = tf.placeholder(tf.float64, name='y') # 教師データ\n",
    "\n",
    "    # パラメータ\n",
    "    a = tf.get_variable('a', initializer=zero)\n",
    "    b = tf.get_variable('b', initializer=zero)\n",
    "\n",
    "    # モデル\n",
    "    pred = tf.identity(a * x + b, name='pred') # 予測値f(x)\n",
    "\n",
    "    # 二乗誤差\n",
    "    loss = tf.div(tf.squared_difference(y, pred), 2, name='loss')\n",
    "\n",
    "    # 勾配降下法による最適化\n",
    "    lr = 0.1 # 学習率\n",
    "    opt = tf.train.GradientDescentOptimizer(lr) # 勾配降下法\n",
    "    fit = opt.minimize(loss)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # パラメータの初期化\n",
    "\n",
    "    for i in range(10):\n",
    "        for _x, _y in sample:\n",
    "            # 最適化の実行\n",
    "            sess.run([fit], feed_dict={\n",
    "                x: _x,\n",
    "                y: _y,\n",
    "            })\n",
    "        # パラメータの取得\n",
    "        [_a, _b] = sess.run([a,b])\n",
    "        print('a={}, b={}'.format(_a,_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じようにほぼ$a=2,b=3$になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "実際のところ、現実での問題は$ax+b$などという直線の簡単な数式では表せません。それこそ、サンプルデータを集めて人の目で見ても理解が難しいものもあります。こういったものを機械的に学習する方法の1つに、ニューラルネットワークをモデルに用いる方法があります。\n",
    "\n",
    "ニューラルネットワークは、生物に見られる神経細胞(ニューロン)のネットワークを模した巨大な演算関数です。十分に深い(多層な)ニューラルネットワークはより複雑なあらゆる関数を近似可能であることが知られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1つの基本的なニューロンは、以下のような数式で表されます\n",
    "\n",
    "$$ \\phi(Wx+b) $$\n",
    "\n",
    "入力が$x$です。$W$は重みパラメータ、$b$はバイアスパラメータです。$W$と$b$はいわゆる定数項です。$Wx+b$の部分は、$W$と$x$の行列積で、バイアスが加算されます。この部分が線形変換であるのに対し、$\\phi$関数は非線形関数が用いられます。\n",
    "\n",
    "複数のニューロンの集合を層(layer)とみなした場合も、$\\phi(Wx+b)$で表されます。この場合、$W,x,b$は全てベクトルまたは行列です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doodleでは、以下のようなニューラルネットワークが記述されています。\n",
    "\n",
    "```python\n",
    "with tf.variable_scope('model', initializer=initializer):\n",
    "    x = image\n",
    "    x = tf.layers.conv2d(x, 32, 5, padding='SAME', activation=tf.nn.relu)\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, padding='SAME')\n",
    "    x = tf.layers.conv2d(x, 64, 5, padding='SAME', activation=tf.nn.relu)\n",
    "    x = tf.layers.max_pooling2d(x, 2, 2, padding='SAME')\n",
    "    x = tf.reshape(x, [-1,7*7*64])\n",
    "    x = tf.layers.dense(x, 1024, activation=tf.nn.relu)\n",
    "    x = tf.layers.dropout(x, rate=dropout_rate, training=is_training)\n",
    "    x = tf.layers.dense(x, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、幾つかの層が使われています。\n",
    "\n",
    "### `tf.layers.dense`\n",
    "\n",
    "これは、密結合層と呼ばれる、ニューラルネットワークで最も基本的なニューロンの集合です。これは、$\\phi(xW+b)$です。\n",
    "\n",
    "上記の`tf.layers.dense(x, 1024, activation=tf.nn.relu)`では、$W$が`[7*7*64, 1024]`の行列、$b$が`[1024]`のベクトル、$\\phi=\\text{relu}$という意味になります。$\\text{relu}(x)=max(x,0)$です。\n",
    "\n",
    "### `tf.layers.conv2d`\n",
    "\n",
    "畳み込み層と呼ばれます。画像認識における最頻出関数です。重みと入力の行列積の代わりに畳み込み演算を行います。$\\phi(\\text{conv2d}(x,W)+b)$です。\n",
    "\n",
    "![](img/b.2.png)\n",
    "\n",
    "`tf.layers.conv2d(x, 32, 5, padding='SAME', activation=tf.nn.relu)`の場合、5x5のカーネルサイズの32枚のフィルタ(実際には、上記図ではカラーチャネルの次元が省略されており、実体は5x5x1x32の4次元テンソルで表される重みパラメータです)を、ストライド長(デフォルトの1)ずつずらしながら入力に対し内積の総和を計算していきます。\n",
    "\n",
    "### `tf.layers.max_pooling2d`\n",
    "\n",
    "プーリング層と呼ばれます。指定した範囲内(2)をストライド長(2)ずつずらしながらに、その中の最大値だけを選んでデータを集約します。画像のダウンサンプリングの効果があります。\n",
    "\n",
    "![](img/b.3.png)\n",
    "\n",
    "### `tf.layers.dropout`\n",
    "\n",
    "ドロップアウトと呼ばれます。学習時にパラメータを`rate`の確率で無効化します。学習データの偏りによる過学習を抑える効果があります。\n",
    "\n",
    "論文:\n",
    "\n",
    "> Srivastava, Nitish, et al. \"Dropout: A simple way to prevent neural networks from overfitting.\" The Journal of Machine Learning Research 15.1 (2014): 1929-1958."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差の定義\n",
    "\n",
    "最初の勾配降下法の例と同様に誤差を定義します。Doodleでは`sparse_softmax_cross_entropy`という関数を用いています。\n",
    "\n",
    "```python\n",
    "with tf.variable_scope('losses'):\n",
    "    # クロスエントロピーを計算して誤差に追加します\n",
    "    cross_entropy_loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=labels, logits=logits)\n",
    "\n",
    "    # モデルで追加された全ての誤差の総和を取得します\n",
    "    total_loss = tf.losses.get_total_loss()\n",
    "```\n",
    "\n",
    "これは、ネットワークの出力を`softmax`関数で離散確率分布に変換し、教師データとの交差エントロピーを計算します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax`関数は、以下の式で表されます。\n",
    "\n",
    "$$ \\text{softmax}(x) = {exp(x) \\over ∑_{j} exp(x_j)} $$\n",
    "\n",
    "引数のベクトル`x`に指数関数を適用し、その総和で割ります。総和で割るということは、出力の総和は必ず1になります。そのため、モデルの出力を確率分布と見なして扱いたい場合に頻繁に用いられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交差エントロピーは、教師データ$y$と予測$y'$に対して以下の式で表されます。ただし、対数関数が使われているため、数式をそのまま実装すると$y'_i=0$の場合に計算不可能になってしまいます。実際には計算的に安定するように実装されている`sparse_softmax_cross_entropy`のような関数を使うのが望ましいでしょう。\n",
    "\n",
    "$$ H_{y}(y') = -\\sum_i y_i \\log(y'_i) $$\n",
    "\n",
    "教師データは、内部的にOne-Hot Vector(OHV)で扱われます。これは引数`labels`の表す数字をインデクスとしたときに、そのインデクスだけが1であるような確率分布です。以下に例を示します。\n",
    "\n",
    "```\n",
    "label=2:\n",
    "[0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "label=9:\n",
    "[0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "label=3:\n",
    "[0,0,0,1,0,0,0,0,0,0]\n",
    "```\n",
    "\n",
    "OHVを確率分布と見なすと、そのインデクスである確率が100%の離散確率分布と捉えられます。モデルの出力である確率分布と、このOHV(答えの確率が100%になっている確率分布)との誤差を計算しています。\n",
    "\n",
    "交差エントロピーは、2つの情報量がどれだけ異なっているのかを表す関数で、2つの確率分布が完全に一致する場合は0となり最小値をとります。2つの確率分布の値がかけ離れるほどに、値は大きくなります。すなわち最初の例のようにこの誤差を最小化するようにニューラルネットワークのパラメータを更新することでモデルを学習することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化\n",
    "\n",
    "誤差を最小化するようにパラメータを最適化するとき、TensorFlowでは自分で微分を計算する必要はありません。\n",
    "\n",
    "```python\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.variable_scope('optimizer'), tf.control_dependencies(update_ops):\n",
    "    # total_loss(誤差の総和)が小さくなるようにパラメータを更新します\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    fit = optimizer.minimize(total_loss, global_step)\n",
    "```\n",
    "\n",
    "`tf.train.◯◯Optimizer`というクラスを使うと、自動でバックプロパゲーションが行われ、`minimize`に指定した誤差が小さくなるように、定義した学習対象のパラメータを自動で更新してくれます。最初に用いた(最急)勾配降下法をベースとした、より発展的な様々なアルゴリズムがサポートされています。Doodleの例では、Adamというアルゴリズムを利用しています。\n",
    "\n",
    "論文:\n",
    "\n",
    "> KINGMA, Diederik P.; BA, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
